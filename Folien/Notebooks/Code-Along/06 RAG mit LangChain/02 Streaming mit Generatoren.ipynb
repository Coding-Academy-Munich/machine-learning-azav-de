{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26b1a1c67e25906",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Streaming mit Generatoren</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias Hölzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17a3c6b16725ac",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Warum Streaming?\n",
    "\n",
    "**Problem:** LLM-Antworten können lang sein\n",
    "\n",
    "- Der Benutzer wartet... und wartet...\n",
    "- Erst am Ende erscheint die komplette Antwort\n",
    "- Das fühlt sich langsam an!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcad8b8ee342c3c",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Die Lösung: Streaming\n",
    "\n",
    "- Zeige die Antwort **während sie generiert wird**\n",
    "- Wie ein Brief, den man Wort für Wort liest\n",
    "- Der Benutzer sieht sofort, dass etwas passiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c255620ec8c08",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Das Schlüsselwort `yield`\n",
    "\n",
    "Normale Funktionen mit `return`:\n",
    "\n",
    "- Funktion gibt **einen** Wert zurück\n",
    "- Funktion **endet** sofort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346d5ec880a7a95",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def normal_function():\n",
    "    return \"Erster Wert\"\n",
    "    return \"Zweiter Wert\"  # Wird nie erreicht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b377af85ae97d80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb6ebe4951628",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Funktionen mit `yield`\n",
    "\n",
    "- Funktion gibt einen Wert zurück und **pausiert**\n",
    "- Bei der nächsten Anfrage macht sie **weiter**\n",
    "- Kann **mehrere Werte** nacheinander liefern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916267b13745a213",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def generator_function():\n",
    "    yield \"Erster Wert\"\n",
    "    yield \"Zweiter Wert\"\n",
    "    yield \"Dritter Wert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01359f974b8a9d2",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Generator-Objekte\n",
    "\n",
    "Ein Aufruf einer Generator-Funktion gibt ein **Generator-Objekt** zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25f7f3c663cd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e3781e40937ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a184652c39a90d",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Werte aus einem Generator holen\n",
    "\n",
    "Mit einer `for`-Schleife können wir alle Werte durchlaufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bbb07363c4928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6c448529b01ed3",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Generatoren in Gradio\n",
    "\n",
    "Gradio unterstützt Generator-Funktionen automatisch!\n",
    "\n",
    "- Bei jedem `yield` wird die Chat-Oberfläche aktualisiert\n",
    "- Der Benutzer sieht die Antwort wachsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d158880cae59f",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Das Muster für Streaming\n",
    "\n",
    "```python\n",
    "def streaming_chat(message, history):\n",
    "    full_response = \"\"\n",
    "    for chunk in ...:  # Chunks vom LLM\n",
    "        full_response += chunk\n",
    "        yield full_response  # Zeige bisherige Antwort\n",
    "```\n",
    "\n",
    "Wichtig: Wir geben immer die **gesamte bisherige Antwort** zurück!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3ba31e053bb22",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Streaming mit LangChain\n",
    "\n",
    "LangChain bietet `llm.stream()` statt `llm.invoke()`:\n",
    "\n",
    "- Gibt Chunks zurück (kleine Teile der Antwort)\n",
    "- Jeder Chunk hat `.content` mit dem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b64f2c0a5ae6e",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35827a2adb266e",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dcdd8163bafd9",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"mistralai/ministral-14b-2512\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3daa6708a61254",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## Beispiel: Streaming in Aktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ccbc4d3c8f70c",
   "metadata": {
    "lang": "de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e895b0342747617e",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Streaming-Chatbot mit Gradio\n",
    "\n",
    "Jetzt kombinieren wir alles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67c03f6db0723c",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e83c88be06ebb",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "chatbot_instances = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfde9388f01b576",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "class FlexibleChatbot:\n",
    "    \"\"\"A chatbot that can use different providers.\"\"\"\n",
    "\n",
    "    def __init__(self, provider=\"openrouter\", system_prompt=None):\n",
    "        self.llm = self._create_llm(provider)\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append(SystemMessage(content=system_prompt))\n",
    "\n",
    "    def _create_llm(self, provider):\n",
    "        \"\"\"Create LLM based on provider name.\"\"\"\n",
    "        if provider == \"openrouter\":\n",
    "            return ChatOpenAI(\n",
    "                api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                model=\"mistralai/ministral-14b-2512\",\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            return ChatOpenAI(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model=\"gpt-4o-mini\",\n",
    "            )\n",
    "        elif provider == \"anthropic\":\n",
    "            return ChatAnthropic(\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                model=\"claude-haiku-4-5\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        self.messages.append(HumanMessage(content=user_message))\n",
    "        response = self.llm.invoke(self.messages)\n",
    "        self.messages.append(response)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f083d782fc5c7de",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def streaming_chat(message, _history, provider):\n",
    "    \"\"\"Chatbot with streaming responses.\"\"\"\n",
    "    global chatbot_instances\n",
    "\n",
    "    if provider not in chatbot_instances:\n",
    "        chatbot_instances[provider] = FlexibleChatbot(\n",
    "            provider, \"Du bist ein hilfreicher Assistent.\"\n",
    "        )\n",
    "\n",
    "    bot = chatbot_instances[provider]\n",
    "    bot.messages.append(HumanMessage(content=message))\n",
    "\n",
    "    full_response = \"\"\n",
    "    for chunk in bot.llm.stream(bot.messages):\n",
    "        if chunk.content:\n",
    "            full_response += chunk.content\n",
    "            yield full_response\n",
    "\n",
    "    bot.messages.append(AIMessage(content=full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff7c4a51e1be8f",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b920362be9dfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30442775891fb3ec",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Workshop: Streaming-Chatbot erweitern\n",
    "\n",
    "**Aufgabe**: Erweitern Sie den Streaming-Chatbot!\n",
    "\n",
    "1. Fügen Sie einen System-Prompt-Auswahl hinzu (wie im vorherigen Workshop)\n",
    "2. Der Benutzer soll sowohl Provider als auch System-Prompt wählen können\n",
    "3. **Bonus**: Zeigen Sie an, wie viele Zeichen bereits generiert wurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbdae15f3d4d8e",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = {\n",
    "    \"Helpful Assistant\": \"You are a helpful assistant.\",\n",
    "    \"Python Tutor\": \"You are a friendly Python tutor who explains concepts simply.\",\n",
    "    \"Pirate\": \"You are a pirate. Answer all questions like a pirate would.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25563c1a1f5c7704",
   "metadata": {
    "tags": [
     "start"
    ]
   },
   "outputs": [],
   "source": [
    "def extended_streaming_chat(message, _history, provider, system_prompt_name):\n",
    "    \"\"\"Streaming chatbot with provider and system prompt selection.\"\"\"\n",
    "    # TODO: Implementieren Sie den Streaming-Chatbot\n",
    "    # Hint: Erstellen Sie für jede Kombination aus Provider und System-Prompt\n",
    "    #       eine eigene Chatbot-Instanz\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0712fd6590b65bf",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "start",
     "subslide"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Erstellen Sie das ChatInterface mit beiden Dropdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee69890cac3fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0133375228cff1b9",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "- **`yield`** pausiert eine Funktion und gibt einen Wert zurück\n",
    "- **Generator-Funktionen** können mehrere Werte nacheinander liefern\n",
    "- **Gradio** aktualisiert die UI bei jedem `yield`\n",
    "- **`llm.stream()`** liefert die Antwort in kleinen Chunks\n",
    "- **Streaming** macht Chatbots reaktiver und benutzerfreundlicher"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "lang,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
