{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5242c920cd2a17",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Richtung moderne NLP</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias Hölzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e05a5095cb1e6c",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Was wir bisher gelernt haben\n",
    "\n",
    "- **Tokenisierung**: Text → Wörter\n",
    "- **Embeddings**: Wörter → dichte Vektoren\n",
    "- **Bag of Words / TF-IDF**: Einfache Text-Repräsentation\n",
    "- **Klassifikation**: Mit Standard ML-Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca7dd635b49290",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd3d6249a7a8c3",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f3cfa3a11b86f",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Das fehlende Stück: Reihenfolge\n",
    "\n",
    "- Bag of Words **ignoriert** die Reihenfolge\n",
    "- Aber Reihenfolge ist wichtig!\n",
    "- \"The dog bit the man\" ≠ \"The man bit the dog\"\n",
    "- \"not good\" ≠ \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d4ebb5de4649",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Sequenzielle Modelle (Konzeptionell)\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "- Verarbeiten Text Wort für Wort\n",
    "- Behalten \"Gedächtnis\" über bisherigen Text\n",
    "- Problem: Vergessen ältere Informationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07006409e4b82ce7",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_rnn_concept():\n",
    "    \"\"\"Conceptual visualization of RNN processing\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    words = [\"The\", \"dog\", \"bit\", \"the\", \"man\"]\n",
    "    x_positions = np.arange(len(words)) * 2.5\n",
    "\n",
    "    # Draw RNN cells\n",
    "    for i, (word, x) in enumerate(zip(words, x_positions)):\n",
    "        # Cell\n",
    "        rect = plt.Rectangle((x, 1), 1.5, 1.2, facecolor='lightblue',\n",
    "                            edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.75, 1.6, word, ha='center', va='center',\n",
    "               fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Arrow to next cell (memory/hidden state)\n",
    "        if i < len(words) - 1:\n",
    "            ax.arrow(x + 1.6, 1.6, 0.7, 0,\n",
    "                    head_width=0.2, head_length=0.1,\n",
    "                    fc='red', ec='red', linewidth=2)\n",
    "            ax.text(x + 2.0, 2.1, 'memory', ha='center',\n",
    "                   fontsize=9, color='red', style='italic')\n",
    "\n",
    "    ax.set_xlim(-0.5, x_positions[-1] + 2)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('RNN: Processing Text Sequentially (with Memory)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520eb97023722b0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c32b3550b12dfe",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Das Aufmerksamkeits-Problem\n",
    "\n",
    "- Beim Lesen eines langen Textes:\n",
    "- Welche Wörter sind wichtig für das aktuelle Wort?\n",
    "- RNNs: Behandeln alle bisherigen Wörter gleich\n",
    "- Lösung: **Attention (Aufmerksamkeit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06edfff8f5977e32",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Attention-Mechanismus (Konzept)\n",
    "\n",
    "- **Idee**: Konzentriere dich auf relevante Wörter\n",
    "- Jedes Wort schaut auf alle anderen Wörter\n",
    "- Lernt automatisch, welche wichtig sind\n",
    "- Beispiel: Bei \"Der König grüßt die Königin\" ...\n",
    "  - \"König\" achtet auf \"grüßt\" und \"Königin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd7557d104c5f1",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_attention_concept():\n",
    "    \"\"\"Conceptual visualization of attention\"\"\"\n",
    "    words = [\"The\", \"king\", \"greets\", \"the\", \"queen\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Position words in circle\n",
    "    n = len(words)\n",
    "    angles = np.linspace(0, 2*np.pi, n, endpoint=False)\n",
    "    x = np.cos(angles)\n",
    "    y = np.sin(angles)\n",
    "\n",
    "    # Draw words\n",
    "    for i, (word, xi, yi) in enumerate(zip(words, x, y)):\n",
    "        circle = plt.Circle((xi, yi), 0.15, facecolor='lightblue',\n",
    "                          edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(xi, yi, word, ha='center', va='center',\n",
    "               fontsize=11, fontweight='bold')\n",
    "\n",
    "    # Draw attention lines from \"king\" to other words\n",
    "    king_idx = 1\n",
    "    attention_weights = [0.1, 0.0, 0.6, 0.1, 0.7]  # Attention to each word\n",
    "\n",
    "    for i in range(n):\n",
    "        if i != king_idx and attention_weights[i] > 0:\n",
    "            # Line width proportional to attention\n",
    "            linewidth = attention_weights[i] * 5\n",
    "            ax.plot([x[king_idx], x[i]], [y[king_idx], y[i]],\n",
    "                   'r-', linewidth=linewidth, alpha=0.6)\n",
    "\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Attention: \"king\" focuses on relevant words\\n(Thicker lines = More attention)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a4b09439967b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a7fb8308fd534bc",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Transformers: Die Revolution\n",
    "\n",
    "- **2017**: \"Attention is All You Need\" (Google)\n",
    "- Verwendet **nur** Attention (keine RNNs)\n",
    "- Viel schneller zu trainieren\n",
    "- Kann längere Texte verarbeiten\n",
    "- **Grundlage für alle modernen LLMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05893ba8deb8c6b",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Der Weg zu LLMs\n",
    "\n",
    "1. **Word Embeddings** (Word2Vec, GloVe)\n",
    "2. **RNNs / LSTMs** (Sequenzielle Verarbeitung)\n",
    "3. **Attention-Mechanismus**\n",
    "4. **Transformers** (Nur Attention)\n",
    "5. **BERT, GPT** (Große vortrainierte Transformers)\n",
    "6. **ChatGPT, Claude** (Moderne LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1603bd039007d",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_nlp_evolution():\n",
    "    \"\"\"Plot evolution of NLP approaches\"\"\"\n",
    "    approaches = [\n",
    "        'Bag of\\nWords',\n",
    "        'Word\\nEmbeddings',\n",
    "        'RNNs/\\nLSTMs',\n",
    "        'Attention',\n",
    "        'Transformers',\n",
    "        'Modern\\nLLMs'\n",
    "    ]\n",
    "\n",
    "    years = [2000, 2013, 2014, 2015, 2017, 2020]\n",
    "    complexity = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot points and line\n",
    "    ax.plot(years, complexity, 'o-', markersize=15, linewidth=3, color='steelblue')\n",
    "\n",
    "    # Add labels\n",
    "    for i, (year, comp, name) in enumerate(zip(years, complexity, approaches)):\n",
    "        ax.text(year, comp + 0.3, name, ha='center', va='bottom',\n",
    "               fontsize=11, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "    ax.set_xlabel('Year (approximate)', fontsize=13)\n",
    "    ax.set_ylabel('Capability / Complexity', fontsize=13)\n",
    "    ax.set_title('Evolution of NLP: From Bag of Words to LLMs',\n",
    "                fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.set_ylim(0, 7)\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabb8782a18f29e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e17911e8e91982b",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Was macht LLMs besonders?\n",
    "\n",
    "- **Riesige Modelle**: Milliarden von Parametern\n",
    "- **Riesige Datenmengen**: Trainiert auf dem halben Internet\n",
    "- **Transfer Learning**: Ein Modell, viele Aufgaben\n",
    "- **Few-Shot Learning**: Lernt aus wenigen Beispielen\n",
    "- **Generierung**: Kann neue Texte erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e50b90fa5eb3da",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Größe der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107486d7ed16cad0",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_model_sizes():\n",
    "    \"\"\"Plot comparison of model sizes\"\"\"\n",
    "    models = ['Small\\nNN\\n(2010)', 'BERT\\n(2018)', 'GPT-2\\n(2019)', 'GPT-3\\n(2020)', 'GPT-4\\n(2023)']\n",
    "    parameters = [1, 340, 1500, 175000, 1000000]  # In millions (GPT-4 estimated)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']\n",
    "    bars = ax.bar(models, parameters, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, param in zip(bars, parameters):\n",
    "        height = bar.get_height()\n",
    "        if param >= 1000:\n",
    "            label = f'{param/1000:.0f}B'\n",
    "        else:\n",
    "            label = f'{param}M'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "               label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Parameters (logarithmic scale)', fontsize=12)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title('Growth of Language Model Size', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c40b7f2bbe08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e70083bbdfc29a",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "- Von **Bag of Words** zu **Transformers**\n",
    "- **Attention**: Fokussiere auf relevante Wörter\n",
    "- **Transformers**: Nur Attention, sehr effizient\n",
    "- **LLMs**: Riesige Transformer-Modelle\n",
    "- Können **generieren**, **übersetzen**, **zusammenfassen**, etc.\n",
    "- Die Revolution der letzten Jahre!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a0d3e596c35b9",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## In der nächsten Lektion\n",
    "\n",
    "- **Large Language Models** im Detail\n",
    "- Wie funktionieren sie?\n",
    "- Was können sie?\n",
    "- Wie nutzt man sie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e03423667dbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,lang,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
