{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91fc2ed9cfe92ea",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Was sind Large Language Models?</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias Hölzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4edd147e9386a6",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Die Revolution der letzten Jahre\n",
    "\n",
    "- **ChatGPT** (Ende 2022): Über 100 Millionen Nutzer in 2 Monaten\n",
    "- **Large Language Models (LLMs)** überall\n",
    "- Können Texte verstehen, generieren, übersetzen, zusammenfassen\n",
    "- Aber was genau sind sie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8212f11d7690f3",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plot_utils import (plot_llm_timeline, plot_llm_capabilities,\n",
    "                        plot_training_data_sources, plot_training_stages,\n",
    "                        plot_emergent_abilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd3d6249a7a8c3",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0d303768d223d",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Definition: Large Language Model\n",
    "\n",
    "- **Large**: Milliarden von Parametern\n",
    "- **Language**: Auf Text trainiert\n",
    "- **Model**: Neuronales Netz (Transformer)\n",
    "- Trainiert auf riesigen Textmengen (Bücher, Websites, etc.)\n",
    "- Lernt Muster, Grammatik, Fakten, Logik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ca8a9089654ac",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Bekannte LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37586f6a3d3b62a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd7625794534e400",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Was können LLMs?\n",
    "\n",
    "- **Text-Generierung**: Artikel, Geschichten, Code schreiben\n",
    "- **Übersetzung**: Zwischen Sprachen übersetzen\n",
    "- **Zusammenfassung**: Lange Texte zusammenfassen\n",
    "- **Fragen beantworten**: Auf Wissensfragen antworten\n",
    "- **Code schreiben**: Programmier-Aufgaben lösen\n",
    "- **Konversation**: Natürliche Gespräche führen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc59d8f26a1d2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30493fec636d185c",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Wie groß ist \"Large\"?\n",
    "\n",
    "- **GPT-2** (2019): 1.5 Milliarden Parameter\n",
    "- **GPT-3** (2020): 175 Milliarden Parameter\n",
    "- **GPT-4** (2023): ~1 Billion Parameter (geschätzt)\n",
    "- Zum Vergleich:\n",
    "  - Menschliches Gehirn: ~86 Milliarden Neuronen\n",
    "  - Aber: Parameter ≠ Neuronen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5894273d87304",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Training eines LLM\n",
    "\n",
    "- **Riesige Datenmengen**: Terabytes von Text\n",
    "- **Rechenleistung**: Tausende GPUs über Monate\n",
    "- **Kosten**: Millionen von Dollar\n",
    "- **Ziel**: Nächstes Wort vorhersagen\n",
    "- \"The cat sat on the ___\" → \"mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb52108e8f03cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc495ea60090c7a",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Pre-Training vs. Fine-Tuning\n",
    "\n",
    "- **Pre-Training**: Auf allgemeinen Daten trainieren\n",
    "  - Lernt Sprache, Fakten, Logik\n",
    "  - Sehr teuer\n",
    "- **Fine-Tuning**: Auf spezielle Aufgaben anpassen\n",
    "  - Konversation, Instruktionen befolgen\n",
    "  - Viel günstiger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295adadaa83ab2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4fe8d206856301",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Emergent Abilities (?)\n",
    "\n",
    "Angeblich:\n",
    "\n",
    "- Bei einer gewissen Größe: **Neue Fähigkeiten tauchen auf**\n",
    "- Kleinere Modelle können es nicht\n",
    "- Größere Modelle plötzlich schon!\n",
    "- Beispiele:\n",
    "  - Mathematik\n",
    "  - Logisches Schließen\n",
    "  - Multi-Step-Reasoning\n",
    "\n",
    "Allerdings umstritten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88304ca6cd610c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19c85f5681f416e",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Limitierungen von LLMs\n",
    "\n",
    "- **Halluzinationen**: Erfinden manchmal Fakten\n",
    "- **Wissens-Cutoff**: Wissen nur bis zu einem bestimmten Datum\n",
    "- **Kein echtes Verständnis**: Muster-Matching, nicht \"Denken\"\n",
    "- **Bias**: Vorurteile aus Trainingsdaten\n",
    "- **Rechenintensiv**: Brauchen viel Energie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec3d0b0c4b9004",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "- **LLMs**: Riesige neuronale Netze für Text\n",
    "- **Basierend auf Transformers** (Attention-Mechanismus)\n",
    "- **Trainiert auf Milliarden von Wörtern**\n",
    "- **Können viele Aufgaben**: Schreiben, Übersetzen, Fragen beantworten\n",
    "- **Emergent Abilities**: Neue Fähigkeiten bei größeren Modellen\n",
    "- **Limitierungen**: Halluzinationen, Bias, kein echtes Verständnis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2fcc8b741e0ce",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## In der nächsten Lektion\n",
    "\n",
    "- **Wie funktionieren LLMs** genauer?\n",
    "- Text-Generierung Schritt für Schritt\n",
    "- Der Transformer-Mechanismus (vereinfacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e03423667dbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,lang,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
