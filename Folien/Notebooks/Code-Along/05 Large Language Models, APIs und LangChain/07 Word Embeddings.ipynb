{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be575de8fadd90c4",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Word Embeddings</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias Hölzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254333cf5093893",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Das Problem mit One-Hot Encoding\n",
    "\n",
    "- Erinnerung: One-Hot ist ineffizient und bedeutungslos\n",
    "- 10.000 Wörter = 10.000-dimensionaler Vektor\n",
    "- \"king\" und \"queen\" sind genau so verschieden wie \"king\" und \"apple\"\n",
    "- Wir brauchen **bessere** Wort-Repräsentationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f89ea34ca94ef5",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd3d6249a7a8c3",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cd1e61876a9e3",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Die Lösung: Word Embeddings\n",
    "\n",
    "- **Embedding** = Dichte Vektor-Repräsentation\n",
    "- Statt 10.000 Dimensionen: Nur 50-300\n",
    "- **Wichtig**: Ähnliche Wörter haben ähnliche Vektoren!\n",
    "- Lernt **Bedeutung** von Wörtern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f28ff33066ec6b",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "\n",
    "## One-Hot vs. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c865f07c3fa049c",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_onehot_vs_embedding():\n",
    "    \"\"\"Compare one-hot and embedding representations\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # One-hot (sparse)\n",
    "    onehot = np.zeros(10)\n",
    "    onehot[3] = 1\n",
    "    ax1.bar(range(10), onehot, alpha=0.7, color='lightblue')\n",
    "    ax1.set_xlabel('Dimension', fontsize=12)\n",
    "    ax1.set_ylabel('Value', fontsize=12)\n",
    "    ax1.set_title('One-Hot Vector\\n(10,000 dimensions, mostly zeros)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylim(0, 1.2)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Embedding (dense)\n",
    "    embedding = np.array([0.2, -0.5, 0.8, 0.1, -0.3])\n",
    "    ax2.bar(range(5), embedding, alpha=0.7, color='lightcoral')\n",
    "    ax2.set_xlabel('Dimension', fontsize=12)\n",
    "    ax2.set_ylabel('Value', fontsize=12)\n",
    "    ax2.set_title('Embedding Vector\\n(50-300 dimensions, dense)', fontsize=12, fontweight='bold')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700b90e099bf9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4fde3d61c2d749",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Beispiel: Word2Vec (konzeptionell)\n",
    "\n",
    "- Eine berühmte Methode für Word Embeddings\n",
    "- Idee: \"You shall know a word by the company it keeps\"\n",
    "- Wörter, die in ähnlichen Kontexten vorkommen, sind ähnlich\n",
    "- Das Netz lernt die Embeddings automatisch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f031d9de0e6912",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Beispiel-Embeddings erstellen\n",
    "\n",
    "- Wir erstellen vereinfachte Embeddings für Demo-Zwecke\n",
    "- In der Praxis: Werden durch Training gelernt\n",
    "- Oder: Vortrainierte Embeddings verwenden (Word2Vec, GloVe, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f41f48680cee32",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "# Create example embeddings (simplified, hand-crafted for demonstration)\n",
    "# Dimensions could represent: [formality, positivity, concreteness, ...]\n",
    "word_embeddings = {\n",
    "    'king': np.array([0.9, 0.3, 0.8]),\n",
    "    'queen': np.array([0.9, 0.3, 0.7]),\n",
    "    'man': np.array([0.5, 0.0, 0.9]),\n",
    "    'woman': np.array([0.5, 0.0, 0.8]),\n",
    "    'prince': np.array([0.8, 0.4, 0.85]),\n",
    "    'princess': np.array([0.8, 0.4, 0.75]),\n",
    "    'apple': np.array([-0.5, 0.2, 1.0]),\n",
    "    'orange': np.array([-0.5, 0.3, 1.0]),\n",
    "    'computer': np.array([0.3, -0.1, 1.0]),\n",
    "    'laptop': np.array([0.3, -0.1, 0.95]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d0b43b5166d37",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3e8a1b7eae1b8a7",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Ähnlichkeit zwischen Wörtern\n",
    "\n",
    "- Wie misst man Ähnlichkeit zwischen Vektoren?\n",
    "- **Cosine Similarity**: Winkel zwischen Vektoren\n",
    "- Wert zwischen -1 und 1\n",
    "- 1 = Identisch, 0 = Orthogonal, -1 = Gegenteilig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61434df11e5410ad",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677740f1153376b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10e79e0551db1df7",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Visualisierung in 2D\n",
    "\n",
    "- Unsere Embeddings haben 3 Dimensionen\n",
    "- Schwer zu visualisieren!\n",
    "- Lösung: **PCA** (Principal Component Analysis)\n",
    "- Reduziert auf 2D für Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a9f3a2213e730",
   "metadata": {
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare data for PCA\n",
    "words = list(word_embeddings.keys())\n",
    "embeddings_matrix = np.array([word_embeddings[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c56ca5677383f",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86dc2df47eb4dd0",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_embeddings_2d(words, embeddings_2d):\n",
    "    \"\"\"Plot word embeddings in 2D space\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot points\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=200, alpha=0.6)\n",
    "\n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=12, fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "    plt.xlabel('First Principal Component', fontsize=12)\n",
    "    plt.ylabel('Second Principal Component', fontsize=12)\n",
    "    plt.title('Word Embeddings Visualization (PCA 2D)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c962cf619eba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f99a10a2985d2f",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Was sehen wir?\n",
    "\n",
    "- **Ähnliche Wörter liegen nahe beieinander!**\n",
    "- Königsfamilie (king, queen, prince, princess) gruppiert\n",
    "- Geschlecht (man, woman) gruppiert\n",
    "- Früchte (apple, orange) gruppiert\n",
    "- Technologie (computer, laptop) gruppiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8185b676fd94733",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Analogien: Die berühmte Gleichung\n",
    "\n",
    "- **king - man + woman ≈ queen**\n",
    "- Vektor-Arithmetik funktioniert!\n",
    "- \"Königlichkeit\" + \"Weiblichkeit\" = \"Königin\"\n",
    "- Das ist die Magie von Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13422256e49abe2",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute analogy\n",
    "analogy_vector = (word_embeddings['king'] -\n",
    "                 word_embeddings['man'] +\n",
    "                 word_embeddings['woman'])\n",
    "\n",
    "# Find closest word\n",
    "def find_closest_word(target_vector, word_embeddings, exclude=[]):\n",
    "    \"\"\"Find word with embedding closest to target vector\"\"\"\n",
    "    best_similarity = -999\n",
    "    best_word = None\n",
    "\n",
    "    for word, embedding in word_embeddings.items():\n",
    "        if word in exclude:\n",
    "            continue\n",
    "        similarity = cosine_similarity(target_vector, embedding)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word, best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220b98766eaf424",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88a076cae44805",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "keep"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_analogy_visualization():\n",
    "    \"\"\"Visualize word analogy as vectors\"\"\"\n",
    "    # Get 2D positions\n",
    "    king_idx = words.index('king')\n",
    "    man_idx = words.index('man')\n",
    "    woman_idx = words.index('woman')\n",
    "    queen_idx = words.index('queen')\n",
    "\n",
    "    king_pos = embeddings_2d[king_idx]\n",
    "    man_pos = embeddings_2d[man_idx]\n",
    "    woman_pos = embeddings_2d[woman_idx]\n",
    "    queen_pos = embeddings_2d[queen_idx]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot all points\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.3, color='gray')\n",
    "\n",
    "    # Highlight relevant points\n",
    "    plt.scatter(*king_pos, s=300, color='blue', label='king', zorder=5)\n",
    "    plt.scatter(*man_pos, s=300, color='red', label='man', zorder=5)\n",
    "    plt.scatter(*woman_pos, s=300, color='green', label='woman', zorder=5)\n",
    "    plt.scatter(*queen_pos, s=300, color='purple', label='queen', zorder=5)\n",
    "\n",
    "    # Draw arrows\n",
    "    # king - man\n",
    "    # plt.arrow(king_pos[0], king_pos[1],\n",
    "    #          -(king_pos[0] - man_pos[0]) * 0.9, -(king_pos[1] - man_pos[1]) * 0.9,\n",
    "    #          head_width=0.01, head_length=0.01, fc='red', ec='red', linewidth=2,\n",
    "    #          length_includes_head=True, label='- man')\n",
    "\n",
    "    # + woman\n",
    "    result_pos = king_pos - (king_pos - man_pos)\n",
    "    # plt.arrow(result_pos[0], result_pos[1],\n",
    "    #          (woman_pos[0] - man_pos[0]) * 0.9, (woman_pos[1] - man_pos[1]) * 0.9,\n",
    "    #          head_width=0.01, head_length=0.01, fc='green', ec='green', linewidth=2,\n",
    "    #          length_includes_head=True, label='+ woman')\n",
    "\n",
    "    # Labels\n",
    "    for i, word in enumerate(words):\n",
    "        if word in ['king', 'man', 'woman', 'queen']:\n",
    "            plt.annotate(word, embeddings_2d[i], fontsize=12, fontweight='bold',\n",
    "                        xytext=(8, 8), textcoords='offset points')\n",
    "\n",
    "    plt.xlabel('First Principal Component', fontsize=12)\n",
    "    plt.ylabel('Second Principal Component', fontsize=12)\n",
    "    plt.title('Word Analogy: king - man + woman ≈ queen', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6693093ac98e21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d8435e3f319a460",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Dimensionen haben Bedeutung\n",
    "\n",
    "- Jede Dimension kodiert ein **Konzept**\n",
    "- Zum Beispiel:\n",
    "  - Dimension 1: Geschlecht (männlich/weiblich)\n",
    "  - Dimension 2: Lebewesen vs. Objekt\n",
    "  - Dimension 3: Konkret vs. Abstrakt\n",
    "- In der Praxis: Nicht so klar interpretierbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888f6fa705c8be9",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Embeddings in der Praxis\n",
    "\n",
    "- **Vortrainierte Embeddings**:\n",
    "  - Word2Vec (Google)\n",
    "  - GloVe (Stanford)\n",
    "  - FastText (Facebook)\n",
    "- Auf riesigen Text-Korpora trainiert\n",
    "- Kann man direkt verwenden oder fine-tunen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa809a381c26d01",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Von Wort-Embeddings zu...\n",
    "\n",
    "- Wort-Embeddings sind **statisch**\n",
    "- Jedes Wort hat **einen** Vektor\n",
    "- Problem: \"bank\" (Geldinstitut) vs. \"bank\" (Flussufer)\n",
    "- Lösung: **Kontextuelle Embeddings**\n",
    "- Jedes Wort bekommt einen Vektor **abhängig vom Kontext**\n",
    "- Das führt uns zu modernen LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676c7b14d8a3716",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "- **Embeddings**: Dichte Vektor-Repräsentationen von Wörtern\n",
    "- **Effizienter**: 50-300 statt 10.000 Dimensionen\n",
    "- **Bedeutungsvoll**: Ähnliche Wörter → ähnliche Vektoren\n",
    "- **Analogien**: Vektor-Arithmetik funktioniert!\n",
    "- **Praktisch**: Vortrainierte Embeddings verfügbar\n",
    "- **Grundlage**: Für moderne NLP und LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198705ff07f52003",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "## In der nächsten Lektion\n",
    "\n",
    "- Wie verarbeitet man **Sequenzen** von Wörtern?\n",
    "- Einfache Modelle für Text-Klassifikation\n",
    "- Praktisches Beispiel mit echten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e03423667dbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,lang,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
